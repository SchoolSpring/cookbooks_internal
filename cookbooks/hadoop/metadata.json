{
  "version": "0.0.1",
  "maintainer_email": "premium@rightscale.com",
  "replacing": {
  },
  "providing": {
  },
  "platforms": {
  },
  "name": "hadoop",
  "recommendations": {
  },
  "attributes": {
    "mapreduce/compile": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Hadoop mapreduce source files to compile.",
      "description": "Source files to complile. Example org/myorg/*.java org/myorg/foo/*.java"
    },
    "mapreduce/input": {
      "type": "string",
      "default": "input",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Hadoop Input Directory",
      "description": "Input directory to copy data"
    },
    "mapreduce/data/storage_account_secret": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Data Storage Account Secret",
      "description": "In order to get the data file to the specified cloud storage location, you will need to provide cloud authentication credentials. For Amazon S3, use your AWS secret access key (e.g., cred:AWS_SECRET_ACCESS_KEY). For Rackspace Cloud Files, use your Rackspace account API key (e.g., cred:RACKSPACE_AUTH_KEY)."
    },
    "mapreduce/data/storage_account_provider": {
      "type": "string",
      "choice": [
        "s3",
        "cloudfiles",
        "cloudfilesuk",
        "SoftLayer_Dallas",
        "SoftLayer_Singapore",
        "SoftLayer_Amsterdam"
      ],
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Data Storage Account Provider",
      "description": "Location where the data file will be retrieved from. Used by dump recipes to back up to Amazon S3 or Rackspace Cloud Files."
    },
    "hadoop/namenode/address/port": {
      "type": "string",
      "default": "8020",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_disallow"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Namenode firewall port",
      "description": "Set the firewall port used by the namenode"
    },
    "mapreduce/cleanup": {
      "type": "string",
      "default": "no",
      "choice": [
        "yes",
        "no"
      ],
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_cleanup"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Delete the destination and Hadoop input/output directories",
      "description": "Removes all the working directories."
    },
    "mapreduce/data/prefix": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_data_import"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Data file name prefix to download",
      "description": "The name that will be used to name/locate the data file.  Should be a .tar.gz file"
    },
    "hadoop/namenode/http/port": {
      "type": "string",
      "default": "50070",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_disallow"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Namenode http firewall port",
      "description": "Set the firewall port used by the namenode http server"
    },
    "mapreduce/command": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Hadoop MapReduce class and arguments to run.",
      "description": "The class and arguments to run the Job.  This input is \n  appended to the end of the 'hadoop jar jarfile' command.  \n  Example: org.myorg.MyMapReduce input output"
    },
    "hadoop/datanode/http/port": {
      "type": "string",
      "default": "50075",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_disallow"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Datanode http firewall port ",
      "description": "Set the firewall port used by the datanode http server"
    },
    "hadoop/node/type": {
      "type": "string",
      "default": "namenode",
      "choice": [
        "namenode",
        "datanode"
      ],
      "recipes": [
        "hadoop::default",
        "hadoop::do_init",
        "hadoop::do_config",
        "hadoop::do_data_import",
        "hadoop::do_cleanup"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Hadoop node type",
      "description": "Hadoop node type, used for managing slaves and masters"
    },
    "rightscale/public_ssh_key": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::default",
        "hadoop::do_init"
      ],
      "required": "required",
      "calculated": false,
      "display_name": "public ssh key ",
      "description": "Hadoop needs a public ssh key which it can use to ssh to \nsystems in it's cluster. This key should also match the private key supplied in ssh/private_ssh_key"
    },
    "mapreduce/data/output_prefix": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Prefix of output file name to upload",
      "description": "The prifix of the output filename.  Should be a .tar.gz file"
    },
    "mapreduce/data/container": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Dump Container",
      "description": "The cloud storage location where the data file will be saved to or restored from. For Amazon S3, use the bucket name. For Rackspace Cloud Files, use the container name."
    },
    "mapreduce/output": {
      "type": "string",
      "default": "output",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Hadoop Output Directory",
      "description": "Output directory to place data after job is done. "
    },
    "hadoop/datanode/address/port": {
      "type": "string",
      "default": "50010",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_disallow"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Datanode address firewall port",
      "description": "Set the firewall port used by the datanode address"
    },
    "mapreduce/data/storage_account_id": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Data Storage Account ID",
      "description": "In order to download the data file to the specified cloud storage location, you need to provide cloud authentication credentials. For Amazon S3, use your Amazon access key ID (e.g., cred:AWS_ACCESS_KEY_ID). For Rackspace Cloud Files, use your Rackspace login username (e.g., cred:RACKSPACE_USERNAME)."
    },
    "mapreduce/name": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Hadoop MapReduce program name",
      "description": "Used as namespace for paths and commands:  Example MyMapReduce"
    },
    "hadoop/dfs/replication": {
      "type": "string",
      "choice": [

      ],
      "recipes": [
        "hadoop::default",
        "hadoop::do_config"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Hadoop namenode dfs.replicaton property ",
      "description": "Hadoop namenode dfs.replicaton property"
    },
    "mapreduce/destination": {
      "type": "string",
      "default": "/mapreduce",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Location of jar file for Hadoop Map Reduce command",
      "description": "Location where data file will be placed."
    },
    "hadoop/datanode/ipc/port": {
      "type": "string",
      "default": "50020",
      "choice": [

      ],
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_disallow"
      ],
      "required": "optional",
      "calculated": false,
      "display_name": "Datanode ipc firewall port ",
      "description": "Set the firewall port used by the datanode ipc address"
    }
  },
  "license": "All rights reserved",
  "groupings": {
  },
  "recipes": {
    "hadoop::do_cleanup": "Remove working directories and mapreduce input/output directories",
    "hadoop::do_map_reduce": "Run MapReduce command.  command and uploads output to cloud provider.",
    "hadoop::do_detach_request": "Detach request",
    "hadoop::do_config": "Configure hadoop",
    "hadoop::default": "Install, configure and init hadoop",
    "hadoop::do_detach_all": "Handle Detach All",
    "hadoop::do_start": "Start Hadoop",
    "hadoop::do_attach_all": "Handle Attach All",
    "hadoop::handle_attach": "Handle Attach",
    "hadoop::do_restart": "Restart Hadoop",
    "hadoop::do_stop": "Stop Hadoop",
    "hadoop::do_init": "Format the namenode",
    "hadoop::do_data_import": "Download data from a cloud provider and copy it into the hadoop FS.",
    "hadoop::do_allow": "Allow connections between cluster hosts",
    "hadoop::do_disallow": "Disallow connections between cluster hosts",
    "hadoop::do_attach_request": "Attach request",
    "hadoop::handle_detach": "Handle Detach",
    "hadoop::install": "Install hadoop"
  },
  "conflicting": {
  },
  "dependencies": {
    "sys_dns": ">= 0.0.0",
    "repo": ">= 0.0.0",
    "sys_firewall": ">= 0.0.0",
    "rightscale": ">= 0.0.0",
    "block_device": ">= 0.0.0"
  },
  "suggestions": {
  },
  "long_description": "Description\n===========\nInstall and Configure Apache Hadoop 1.03\n\nRequirements\n============\nJava\n\n\nAttributes\n==========\n\nUsage\n=====\n\nTo create a Hadoop Cluster, create multiple servers.  Each server will be \ndesignated as a namenode (master) or datanode (slave).  \n\nSet the hadoop/dfs/replication input to the size of your datanode replication size.\n\n\nMapReduce\n\nThere are three recipes for MapReduce \n 1. hadoop::do_data_import - runs hadoop::do_cleanup (if mapreduce/cleanup==yes then downloads data and copies it to the hadoop HDFS\n 2. hadoop::do_map_reduce - downloads MapReduce code from Repository compiles it, \n    runs MapReduce program and uploads it to the ROS\n\nSet MapReduce inputs\n  map",
  "description": "Install and Configure Apache Hadoop",
  "maintainer": "RightScale Inc"
}